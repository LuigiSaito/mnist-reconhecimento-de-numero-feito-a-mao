# -*- coding: utf-8 -*-
"""operation_2thgrade.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19LaYNhOW3jNvS3dehc_398H2T-5_X-zL

# ⚪ Trabalho de Sistemas Inteligentes

- Diego Lopes Ferreira ([Github](https://github.com/Diego-Lopes-Ferreira), [Email](dglpferreira@gmail.com))
- Giovanni Vinicius Rodrigues ([Email](giovannivinicius@alunos.utfpr.edu.br))
- Luigi Lista Saito ([Email](luigisaito@alunos.utfpr.edu.br))
- Lucas Fernando da Silva ([Github](https://github.com/lucasglv), [Email](lucasfernando@alunos.utfpr.edu.br))


### Referências:
O trabalho foi baseado no algoritmo desenvolvido em:

https://www.youtube.com/watch?v=w8yWXqWQYmU&t

https://www.kaggle.com/code/wwsalmon/simple-mnist-nn-from-scratch-numpy-no-tf-keras/notebook

### Guia
- ⚪ Banco de Dados
- ⚪ Definição das Funções da Rede Neural
- ⚪ Validação Cruzada de Hiper Parâmetros
- ⚪ Treinamento da Rede Neural
- ⚪ Bancada de Testes

# ⚪ Banco de Dados

Separação do banco de treinamento em duas partes:
- Dados de treino (utlizados para alterar os pesos sinápticos)
- Dados de teste (utlizados para gerar a acurácia no final do treinamento)
"""

import numpy as np
import pandas as pd
from time import time

treino_dados_completo = pd.read_csv('/content/mnist_train.csv')
# treino_dados_completo = pd.read_csv('/content/sample_data/mnist_test.csv')
treino_dados_completo = np.array(treino_dados_completo) # Vetor com os valores
np.random.shuffle(treino_dados_completo)       # Embaralha os dados para separar em treino e teste

teste_dados = pd.read_csv('/content/sample_data/mnist_test.csv')
teste_dados = np.array(teste_dados) # Vetor com os valores

# Separacao dos dados de treino em "treino", "validacao" e "teste"

m, n = treino_dados_completo.shape
treino_final = treino_dados_completo.T # Transpoe os primeiros elementos do dataset
treino_final_y = treino_final[0]       # Primeira coluna do dataset (primeira linha)
treino_final_x = treino_final[1:n]     # Colunas 1 ate n do dataset (linhas 1 ate n)
treino_final_x = treino_final_x / 255. # Normalizacao dos dados
# print(treino_final_x.shape)
# print(treino_final_y.shape)

m, n = teste_dados.shape
teste_dados = teste_dados.T            # Transpoe os elementos do dataset de teste
teste_saidas = teste_dados[0]          # Primeira coluna do dataset (primeira linha)
teste_entradas = teste_dados[1:n]      # Colunas 1 ate n do dataset (linhas 1 ate n)
teste_entradas = teste_entradas / 255. # Normalizacao dos dados
# print(teste_entradas.shape)
# print(teste_saidas.shape)

"""# ⚪ Definição das funções da rede neural

O próximo passo é definir algumas funções auxiliares.
- A função `one_hot(Y)` retorna um vetor equivalente ao número.
- As funções de ativação `ReLU` e `Softmax`

E as funções da rede
- A inicialização randômica de pesos
- Os algoritmos de _forward_ e _backward propagation_
- O ajuste de pesos pela regra delta
- Cálculo de saída

Além de algumas funções que agrupam as outras com as configurações adequadas
- A fábrica de treinamento da rede
- Cálculo de acurácia
"""

def one_hot(Y):
    '''Converte um numero X para um vetor com zeros e somente um numero 1 na posicao X
    one_hot(2) => [0 0 1 0 0 0 0 0 0 0]
    (Utilizado para gerar o vetor de rotulos)'''
    one_hot_Y = np.zeros((Y.size, Y.max() + 1))
    # Basicamente cria um vetor de zeros do tamanho exato de possiveis classificacoes
    one_hot_Y[np.arange(Y.size), Y] = 1
    one_hot_Y = one_hot_Y.T
    return one_hot_Y

def ReLU(x):
    '''Funcao de Ativacao: ReLU (entre 0 e 1)'''
    # e possivel utilizar outras como as sigmoidais, usamos a linear pois para a aplicacao e suficiente
    return np.maximum(x, 0)

def ReLU_deriv(x):
    '''Derivada da funcao de ativacao ReLu'''
    return x > 0

def softmax(x):
    '''Funcao de Ativacao: Softmax (entre MIN e MAX)'''
    A = np.exp(x) / sum(np.exp(x))
    return A

def rede_inicializacao():
    '''define a topologia(neurionio nas camadas e tal)
    vai ter 784 entradas(para cada valor da matriz da imagem) 10 na escondida e 10 na saida'''
    W1 = np.random.rand(10, 784) - 0.5 # W e o W (n diga) e b o bias, gera valores pequenos para cada entrada
    b1 = np.random.rand(10, 1) - 0.5   # Cria cada camada com 10 neuronio

    W2 = np.random.rand(10, 10) - 0.5
    b2 = np.random.rand(10, 1) - 0.5
    return W1, b1, W2, b2

def rede_forward_propagation(W1, b1, W2, b2, X):
    '''Calcula a saida da rede Y = f( W2 * g( W1 * X +b1 ) +b2 ) '''
    Z1 = W1.dot(X) + b1
    A1 = ReLU(Z1)
    Z2 = W2.dot(A1) + b2
    A2 = softmax(Z2)
    return Z1, A1, Z2, A2

def rede_backward_propagation(Z1, A1, Z2, A2, W1, W2, X, Y):
    '''Calcula os ajustes de pesos da rede (backpropagation na unha)
    O Levenberg-Marquardt backpropagation'''
    m = Y.size # Usado no calculo do erro (para dividir pela quantidade de amostras)
    one_hot_Y = one_hot(Y) # Converte 2 => [0 0 1 0 0 0 0 0 0 0]

    dZ2 = A2 - one_hot_Y
    dW2 = 1 / m * dZ2.dot(A1.T)
    db2 = 1 / m * np.sum(dZ2)

    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)
    dW1 = 1 / m * dZ1.dot(X.T)
    db1 = 1 / m * np.sum(dZ1)
    return dW1, db1, dW2, db2

def rede_atualizacao_pesos(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):
    '''Atualizacao os pesos sinapticos (obtidos pela `backward_propagation`) atravez da regra Delta'''
    W1 = W1 - alpha * dW1
    b1 = b1 - alpha * db1
    W2 = W2 - alpha * dW2
    b2 = b2 - alpha * db2
    return W1, b1, W2, b2

def rede_calcula_saida(Y):
    '''Dada uma matriz de saida Y, retorna um numero: resultado da rede'''
    return np.argmax(Y, 0)

def rede_calcula_acuracia(W1, b1, W2, b2, X, Y):
    '''Dado um vetor de previsoes e um vetor de saidas desejadas, calcula a taxa
    de acertos da rede'''
    _, _, _, rede_y = rede_forward_propagation(W1, b1, W2, b2, X)
    predictions = rede_calcula_saida(rede_y)
    return np.sum(predictions == Y) / Y.size

def rede_treinamento(treino_x, treino_y, validacao_x, validacao_y, alpha=0.10, max_epoch=100, debug=False):
    '''Utiliza os dados fornecidos para treinar a rede em diversas epocas.
    Retorna a acuracia e os dados'''
    W1, b1, W2, b2 = rede_inicializacao()

    for epoca in range(max_epoch):
        # Uma epoca de treinamento:
        Z1, A1, Z2, A2 = rede_forward_propagation(W1, b1, W2, b2, treino_x)
        dW1, db1, dW2, db2 = rede_backward_propagation(Z1, A1, Z2, A2, W1, W2, treino_x, treino_y)
        W1, b1, W2, b2 = rede_atualizacao_pesos(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)

        # Verificacao da acuracia
        acuracia = rede_calcula_acuracia(W1, b1, W2, b2, validacao_x, validacao_y)

        if (debug and (epoca % 50 == 0)): # A cada 50 epocas, mostra a acuracia
            print("  Epoca: %4d | Acuracia: %7.3f %%" % (epoca, acuracia*100))

        if (acuracia > 0.9) and (epoca > 50): # Condicao de parada por acuracia
            if (debug):
                print("  Acuracia alcançada")
                print("  Acuracia: ", acuracia)
            break

    # Ultima verificacao de acuracia:
    acuracia = rede_calcula_acuracia(W1, b1, W2, b2, validacao_x, validacao_y)
    return acuracia, W1, b1, W2, b2

"""# ⚪ Validação Cruzada de Hiper Parâmetros

O processo de validação cruzada da rede neural é realizado através de um K-Fold de 5 dobras.

| `1/5` | `2/5` | `3/5` | `4/5` | `5/5` |
|-------|-------|-------|-------|-------|
| VALIDAÇÃO | TREINO | TREINO | TREINO | TREINO |
| TREINO | VALIDAÇÃO | TREINO | TREINO | TREINO |
| TREINO | TREINO | VALIDAÇÃO | TREINO | TREINO |
| TREINO | TREINO | TREINO | VALIDAÇÃO | TREINO |
| TREINO | TREINO | TREINO | TREINO | VALIDAÇÃO |
"""

# Codigo mostrando como funciona a separacao dos vetores do K-Fold:
lista_exemplo = np.array([0, 1, 2, 3, 4, 5])
start = 1# Altere aqui para entender como funciona
end = start + 1
indexes = np.r_[0:start, end:5]
print(lista_exemplo[indexes])
print(lista_exemplo[start:end])

# Configuracoes:
N_FOLD = 5
ALPHAS = [0.10, 0.15, 0.20]
MAX_EPOCH = 500

# Treinamento
redes = []
m, n = treino_dados_completo.shape
print( 'Amostras com m=%d e n=%d' % (m, n) )
for ALPHA in ALPHAS:
  print('ALPHA: %.3f' % ALPHA)
  FOLD_SIZE = m // N_FOLD # Encontra a quantidade de amostras de treino
  for fold in range(N_FOLD):
      t0 = time()
      # Separacao dos dados do fold
      start = fold * FOLD_SIZE # Ex: se FOLD_SIZE = 100 =>   0, 100, 200, ...
      end = start + FOLD_SIZE  # Ex: se FOLD_SIZE = 100 => 100, 200, 300, ...
      print( 'FOLD: %d (Validacao de %5d a %5d)' % (fold, start, end) )

      validacao = treino_dados_completo[start:end].T
      validacao_y = validacao[0]
      validacao_x = validacao[1:n]
      validacao_x = validacao_x / 255. # Normalizacao

      indexes = np.r_[0:start, end:m]
      treino = treino_dados_completo[indexes].T
      treino_y = treino[0]
      treino_x = treino[1:n]
      treino_x = treino_x / 255. # Normalizacao

      acuracia, W1, b1, W2, b2 = rede_treinamento(treino_x, treino_y, validacao_x, validacao_y, alpha=ALPHA, max_epoch=MAX_EPOCH, debug=False)
      training_time = time() - t0
      rede_atual = [acuracia, training_time, W1, b1, W2, b2, ALPHA]
      redes.append(rede_atual)

print("Fim do Treinamento")

# Mostra os dados das redes encontradas no processo de validacao cruzada
j = 0
soma = 0
for i, rede in enumerate(redes):
    print('Rede %d | %.3f | %10.3f s | Acuracia de %8.4f %%' % (i, rede[6], rede[1], rede[0]*100))
    j += 1
    soma += rede[0]
    if j == 5:
      print('  Media: %.3f' % (100*soma / 5))
      soma = 0
      j = 0

# Carregar uma das redes treinadas na validacao cruzada para o workspace:
INDICE_DA_REDE = 11
acuracia, _, W1, b1, W2, b2, _ = redes[INDICE_DA_REDE]
print( 'Rede %d (%8.4f %%) selecionada' % (INDICE_DA_REDE, acuracia*100) )

"""# ⚪ Treinamento da Rede Neural

Treinamento final realizado com todos os dados do banco de dados de treino (60 000), utilizando como parâmetros de validação o banco de dados de teste (10 000).

> _O tempo médio de treinamento é de 60 segundos_
"""

t0 = time()
acuracia, W1, b1, W2, b2 = rede_treinamento(treino_final_x, treino_final_y, teste_entradas, teste_saidas, alpha=0.20, max_epoch=500, debug=True)
training_time = time() - t0
print('Tempo: %.3f s' % training_time)

acuracia_teste = rede_calcula_acuracia(W1, b1, W2, b2, teste_entradas, teste_saidas)
print('TESTE FINAL: %7.3f %%' % (acuracia_teste * 100))

"""# ⚪ Bancada de Testes

Com a rede já treinada, é possível utilizar o banco de dados mostrar as imagens e o que foi encontrado pela rede.
"""

from matplotlib import pyplot as plt

def show_indices_of_labels(n=5):
    '''Mostra quais sao os indices para cada um dos numeros'''
    indices = [ [], [], [], [], [], [], [], [], [], [] ]
    for i, lbl in enumerate(teste_saidas):
      indices[lbl].append(i)

    for lbl in range(10):
        print('| %d |' % lbl, sep='', end='')
        for i in range(n):
            if i == 0:
                print(' `%3d`' % indices[lbl][i], sep='', end='')
            else:
                print(', `%3d`' % indices[lbl][i], sep='', end='')
        print(' | ')

def rede_mostra_imagem(indice):
    '''Acessa um indice do banco de dados de teste, calcula a saida da rede e mostra a imagem'''
    current_image = teste_entradas[:, indice, None]
    _, _, _, A2 = rede_forward_propagation(W1, b1, W2, b2, current_image)
    prediction = np.argmax(A2)
    label = teste_saidas[indice]
    print("PREVISAO DA REDE NEURAL: ", prediction)
    print("             VALOR REAL: ", label)
    current_image = current_image.reshape((28, 28)) * 255
    plt.gray()
    plt.imshow(current_image, interpolation='nearest')
    plt.show()

# show_indices_of_labels(n=5)

"""Os dados estão embaralhados, então o índice `1` do vetor de testes não necessariamente é nescessariamente o número `1`. Aqui vai uma lista com alguns índices de números para testar:

| Número | Possiveis índices |
|--------|-----------------------|
| 0 | `  2`, `  9`, ` 12`, ` 24`, ` 27` |
| 1 | `  1`, `  4`, ` 13`, ` 28`, ` 30` |
| 2 | `  0`, ` 34`, ` 37`, ` 42`, ` 46` |
| 3 | ` 17`, ` 29`, ` 31`, ` 43`, ` 50` |
| 4 | `  3`, `  5`, ` 18`, ` 23`, ` 26` |
| 5 | `  7`, ` 14`, ` 22`, ` 44`, ` 51` |
| 6 | ` 10`, ` 20`, ` 21`, ` 49`, ` 53` |
| 7 | ` 16`, ` 25`, ` 33`, ` 35`, ` 40` |
| 8 | ` 60`, ` 83`, `109`, `127`, `133` |
| 9 | `  6`, `  8`, ` 11`, ` 15`, ` 19` |





"""

rede_mostra_imagem(2034) # Teste com 7 ou 150 ou 1501 ou 2023 ou 2034